{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245716c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e953827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = ds.MNIST(root=\"\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = ds.MNIST(root=\"\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hid1 = nn.Sequential(nn.Linear(784,512), nn.Tanh())\n",
    "        self.out = nn.Sequential(nn.Linear(512,10), nn.Softmax(dim=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.hid1(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "mlp_sgd = MLP()\n",
    "mlp_adam = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a111f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=mnist_train, batch_size=128, shuffle=True) \n",
    "test_dataloader = DataLoader(dataset=mnist_test, batch_size=128, shuffle=True)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = SGD(mlp_sgd.parameters(), lr=0.01)\n",
    "epochs = 50\n",
    "train_acc_sgd, test_acc_sgd = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss_iter, test_loss_iter = [], []\n",
    "    train_acc_iter, test_acc_iter = [], []\n",
    "    \n",
    "    for X, y in train_dataloader:\n",
    "    \n",
    "        mlp_sgd.train()\n",
    "        y_pred = mlp_sgd(X)\n",
    "        y = nn.functional.one_hot(y, 10).type(torch.FloatTensor)\n",
    "        train_loss = loss(y_pred, y)\n",
    "        train_loss_iter.append(train_loss.item())\n",
    "        train_acc = (torch.argmax(y_pred, axis=1) == torch.argmax(y, axis=1)).sum() / len(y)\n",
    "        train_acc_iter.append(train_acc.item())\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "    \n",
    "        mlp_sgd.eval()\n",
    "        with torch.no_grad():\n",
    "            y = nn.functional.one_hot(y, 10).type(torch.FloatTensor)\n",
    "            y_pred = mlp_sgd(X)\n",
    "            test_loss = loss(y_pred, y)\n",
    "            test_loss_iter.append(test_loss.item())\n",
    "            test_acc = (torch.argmax(y_pred, axis=1) == torch.argmax(y, axis=1)).sum() / len(y)\n",
    "            test_acc_iter.append(test_acc.item())\n",
    "            \n",
    "    train_loss_epoch = round(np.array(train_loss_iter).mean(), 4)\n",
    "    train_acc_epoch = round(np.array(train_acc_iter).mean(), 4)\n",
    "    test_loss_epoch = round(np.array(test_loss_iter).mean(), 4)\n",
    "    test_acc_epoch = round(np.array(test_acc_iter).mean(), 4)\n",
    "    \n",
    "    train_acc_sgd.append(train_acc_epoch)\n",
    "    test_acc_sgd.append(test_acc_epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}\\tloss: {train_loss_epoch}\\taccuracy: {train_acc_epoch} \\\n",
    "    val loss: {test_loss_epoch}\\tval accuracy: {test_acc_epoch}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28600187",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_list = []\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    \n",
    "    mlp_sgd.eval()\n",
    "    with torch.no_grad():\n",
    "        y = nn.functional.one_hot(y, 10).type(torch.FloatTensor)\n",
    "        y_pred = mlp_sgd(X)\n",
    "        test_acc = (torch.argmax(y_pred, axis=1) == torch.argmax(y, axis=1)).sum() / len(y)\n",
    "        test_acc_list.append(test_acc.item())\n",
    "\n",
    "res = np.array(test_acc_list).mean()\n",
    "print(f\"정확률 = {res*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34346730",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=mnist_train, batch_size=128, shuffle=True) \n",
    "test_dataloader = DataLoader(dataset=mnist_test, batch_size=128, shuffle=True)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = Adam(mlp_adam.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "train_acc_adam, test_acc_adam = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss_iter, test_loss_iter = [], []\n",
    "    train_acc_iter, test_acc_iter = [], []\n",
    "    \n",
    "    for X, y in train_dataloader:\n",
    "    \n",
    "        mlp_adam.train()\n",
    "        y_pred = mlp_adam(X)\n",
    "        y = nn.functional.one_hot(y, 10).type(torch.FloatTensor)\n",
    "        train_loss = loss(y_pred, y)\n",
    "        train_loss_iter.append(train_loss.item())\n",
    "        train_acc = (torch.argmax(y_pred, axis=1) == torch.argmax(y, axis=1)).sum() / len(y)\n",
    "        train_acc_iter.append(train_acc.item())\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "    \n",
    "        mlp_adam.eval()\n",
    "        with torch.no_grad():\n",
    "            y = nn.functional.one_hot(y, 10).type(torch.FloatTensor)\n",
    "            y_pred = mlp_adam(X)\n",
    "            test_loss = loss(y_pred, y)\n",
    "            test_loss_iter.append(test_loss.item())\n",
    "            test_acc = (torch.argmax(y_pred, axis=1) == torch.argmax(y, axis=1)).sum() / len(y)\n",
    "            test_acc_iter.append(test_acc.item())\n",
    "            \n",
    "    train_loss_epoch = round(np.array(train_loss_iter).mean(), 4)\n",
    "    train_acc_epoch = round(np.array(train_acc_iter).mean(), 4)\n",
    "    test_loss_epoch = round(np.array(test_loss_iter).mean(), 4)\n",
    "    test_acc_epoch = round(np.array(test_acc_iter).mean(), 4)\n",
    "    \n",
    "    train_acc_adam.append(train_acc_epoch)\n",
    "    test_acc_adam.append(test_acc_epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}\\tloss: {train_loss_epoch}\\taccuracy: {train_acc_epoch} \\\n",
    "    val loss: {test_loss_epoch}\\tval accuracy: {test_acc_epoch}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07773104",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_list = []\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    \n",
    "    mlp_adam.eval()\n",
    "    with torch.no_grad():\n",
    "        y = nn.functional.one_hot(y, 10).type(torch.FloatTensor)\n",
    "        y_pred = mlp_adam(X)\n",
    "        test_acc = (torch.argmax(y_pred, axis=1) == torch.argmax(y, axis=1)).sum() / len(y)\n",
    "        test_acc_list.append(test_acc.item())\n",
    "\n",
    "res = np.array(test_acc_list).mean()\n",
    "print(f\"정확률 = {res*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_sgd, \"r--\")\n",
    "plt.plot(test_acc_sgd, \"r\")\n",
    "plt.plot(train_acc_adam, \"b--\")\n",
    "plt.plot(test_acc_adam, \"b\")\n",
    "plt.title(\"Comparison of SGD and Adam optimizers\")\n",
    "plt.ylim((0.7,1.0))\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend([\"train_sgd\", \"val_sgd\", \"train_adam\", \"val_adam\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
